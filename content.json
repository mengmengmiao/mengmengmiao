[{"title":"MLY -- 11.When to change dev/test sets and metrics","date":"2017-02-17T04:29:01.000Z","path":"2017/02/17/MLY-11-When-to-change-dev-test-sets-and-metrics/","text":"当开始做一个新项目时，我会快速选择开发集和测试集，因为这会给团队一个明确的目标。通常，我会要求我的团队在一周之内提出初始的开发集、测试集和度量，大多时候都不会多于一周。先提出一些不完美的东西使项目能前进下去，比过多考虑开发集、测试集、度量好的多。但是，一周时间线并不适用于成熟的应用。例如，垃圾邮件是一个成熟的深度学习应用。我曾见到在已经很成熟的系统上工作的团队花费数月的时间去好点儿的开发/测试集。如果在后续开发中，你发现初始开发集、测试集、度量错过了标记（missed the mark），一定要快速地改进。例如，如果你的开发集+度量使分类器A得分优于分类器B，但是你的团队认为分类器B实际上对你的产品更好，那么这就可能就是你需要改进你的开发集、测试集或者度量的时候了。1.实际的分布和开发集、测试集的分布不同假设你初始的开发/测试集主要由成年猫的图片组成。当发布了app后，出乎意料，你发现用户会上传很多幼年猫的图片。此时，开发/测试集的分布就不能代表实际的分布了。在这种情况下，你需要更新开发/测试集，使其更具代表性。2.过拟合开发集了重复地用开发集来评价ideas可能导致过拟合开发集。当项目开发结束时，需要在测试集上评估你的系统。如果你发现的开发集的表现比测试集好，那么这可能就是过拟合开发集了。在这种情况下，你需要一个新的开发集（a fresh dev set）。如果你需要跟踪团队的进展，你可以用测试集定期评估你的系统（例如每周一次或每月一次）。但是不要用测试集做任何关于算法的决策，包括是否要回滚到上一个系统。如果你这么做了，系统将会开始过拟合测试集。并且不能再依赖测试集给出系统性能的完全无偏评估（当你发表论文或使用这个评估去做商业决策时就需要完全无偏评估）。3.度量（the metric）中不包括项目需要优化的东西假设对于猫分类器，你选择了分类准确度（classification accuracy）作为度量（metric），在这种度量下，分类器A比分类器B号。但是，当你实际去用这两种算法时，你发现分类器A偶尔会允许色情图片溜过去。即使分类器A准确率更高，但偶尔的一张色情图片会给用户留下坏印象，因此分类器A并不可取。此时，度量未能分辨出算法B实际上比算法A好。因此，度量不再可信，是时候改变度量了。例如，你可以改变度量使其惩罚色情图片通过。我强烈建议选择一个新的度量，并用这个新度量为团队定义一个新目标，而不是在没有可信度量的情况下进行太长时间以至于最后恢复到手动选择分类器。在项目期改变开发/测试集或评价度量时很常见的。初始的开发/测试集能帮助你快速进入迭代期。如果你发现开发/测试集或者度量不再能指引正确的方向，那也没事儿。只要改变他们并且保证你的团队知道新的方向就行了。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 10.Having a dev set and metric speeds up iterations","date":"2017-02-16T09:23:42.000Z","path":"2017/02/16/MLY-10-Having-a-dev-set-and-metric-speeds-up-iterations/","text":"很难事先知道什么算法最适合一个新问题。就算是经验丰富的机器学习研究人员通常也需要尝试一系列方法后才能发现一些令人满意的方法。当构建一个机器学习系统时，我经常：1.以一些如何构建系统的想法开始2.用代码实现这些想法3.做实验，实验会告诉我们哪些想法效果好。（通常，我开始的几个想法效果并不好）基于这些已学到的想法，我们回到第1步再产生更多想法，并循环下去。上图就是迭代过程。这一圈你走的越快，你的进展就越快。这就是为什么开发集和测试集重要：当你尝试一个想法时，你可以在开发集上测量想法的表现，这能使你快速地发现是否你正在朝着正确的方向前进。与此相对的是，假设你没有具体的开发集和度量（metric），每次你的团队开发一个新的猫分类器，就需要将这个分类器合并到你的app中，然后玩上几小时感觉一下新分类器是否有提升。这过程相当慢呀是不是？而且，当你的分类器从95.0%提升到95.1%时，你并不能通过玩app感觉出来这0.1%呀。拥有开发集和度量可以使你快速递检测出哪个想法会使分类集得到一点提升，从而使你快速判断出哪个想法需要再微调，哪个想法需要抛弃。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 9.Optimizing and satisficing metrics","date":"2017-02-16T07:22:04.000Z","path":"2017/02/16/MLY-9-Optimizing-and-satisficing-metrics/","text":"其实，还有一种组合多个评价度量的方法。假设，你将准确率和运行时间看做同等重要的评价项，你需要从以下三个分类器中选择一个： classifier accuracy running time A 90% 80ms B 92% 95ms C 95% 1500ms 如按照上一章的方法，通过将准确率和运行时间放入一个公式中得到单数值度量看起来是不自然的： Accuracy -0.5*RunningTime你可以这样做：首先，确定能接受的运行时间，例如运行时间在100ms内都是可接受的；然后，在满足运行时间这个条件下，最大化准确率。这里运行时间是一个“满意度量”——分类器只需要在“满意度量”下足够好，这意味着，你最多能花费100ms。准确率是“优化度量”。如果你想平衡（trade off）N个评价项，例如模型的二进制文件大小（这对手机app很重要，用户们不会喜欢下载文件大的app）、运行时间、准确率，你可能考虑设置N-1项评价为“满意度量”。换句话说，你只需要这N-1个评价度量们满足一个确定值。在这种情况下，你就可以定义最后一个评价为“优化度量”。例如，为二进制文件大小和运行时间设置一个阈值，然后在这些些约束条件下优化准确率。最后，再举一例。假设你正在构建一个硬件设备：当用户通过麦克风说一个“唤醒词”时，系统被唤醒。例如，Amazon Echo被“Alexa”唤醒，Apple Siri被“Hey Siri”唤醒；Android被“Okay Google”唤醒，Baidu apps被“Hello Baidu”唤醒。你关心的评价项有假正率（false positive rate）——没有人唤系统时系统却唤醒的频率，假负率（false negative rate ）—— 当有人唤系统时系统没被唤醒的频率。开发这个系统时应设置的合理目标是：在每24小时不超过一次假唤醒（false positive）的条件下（满意度量），最小化假负率（优化度量）。一旦你的团队明确了要优化的评价度量，他们将会进展更快。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 8.Establish a single-number evaluation metric for your team to optimize","date":"2017-02-16T05:08:08.000Z","path":"2017/02/16/MLY-8-Establish-a-single-number-evaluation-metric-for-your-team-to-optimize/","text":"分类准确率(classification accuracy)是一个单数值的评价度量：将开发集（或测试集）输入分类器分类，分类器返回“被分类器正确分类的数据占输入数据集的比例”。如果分类器A获得97%的准确率，分类器B获得90%的准确率费，根据评价度量，分类器A较好。与此不同的是，精度率和召回率（precision and recall）就不是一个单数值评价度量：它用两个数值去评价分类器。多数值评价度量使得比较分类算法时变得困难。假设你的分类算法表现如下： classifier precision recall A 95% 90% B 98% 85% 这里，并不能直接看出两个分类器中哪个更好。 在实际开发过程中，你的团队将用不同的算法框架、模型参数、特征选择等。而使用单数值评价度量（如准确率）能快速地对这些算法的优劣做一个排序，从而看出哪一个算法更好。 如果你真的关心精确率和召回率，我建议你使用一种标准的方法将两种评价数值糅合成一个。例如，可取精确率和召回率的平均值作为评价数值；还可以取“F1 score”，F1 score是计算平均值的一种改进的方法，比取平均值的表现好多了。 classifier precision recall F1 score A 95% 90% 92.4% B 98% 85% 91.0% 单数值评价度量能让你从众多算法中快速选出哪个算法最优，它对众多算法进行偏好排序，因而明确了进展的方向。 最后，假设你分别在四个主要市场（(i)US,(ii)China,(iii)India,(iv)其他）跟踪你的猫分类器准确率，因此你得到四个度量。对这四个度量取平均或加权平均值，你就会得到一个评价数值。取平均或加权平均是将多个度量变成一个的最常用的方法。 ##注释： 精确率和召回率：以猫图片分类器为例，精确率是指在分类器分类成猫的图片中，真的是猫图片的比例。召回率是指在开发集（或测试集）中的猫图片被分类器分类成猫图片的概率。在高准确率和高召回率间，经常需要权衡（tradeoff）。 F1 score： 如果你想了解更多，可以点击 https://en.wikipedia.org/wiki/F1_score。F1 score就是精确率和召回率的几何平均值，是这样计算的 2/((1/precision)+(1/recall))。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"PathNet: Evolution Channels Gradient Descent in Super Nerual Networks","date":"2017-02-07T03:09:14.000Z","path":"2017/02/07/PathNet-Evolution-Channels-Gradient-Descent-in-Super/","text":"来自Google DeepMind，Google Brain,1月30，arXiv的一片文章。 摘要：对AGI（artificial general intelligence，强人工智能）来说，若多个用户同时训练巨大的神经网路，允许参数复用，不允许灾难性的遗忘，将会是有效的。","tags":[{"name":"new papers","slug":"new-papers","permalink":"https://mengmengmiao.github.io/tags/new-papers/"}]},{"title":"MLY -- 7.How large do the dev/test sets need to be?","date":"2017-01-30T13:12:33.000Z","path":"2017/01/30/MLY-7-How-large-do-the-dev-test-sets-need-to-be/","text":"开发集应该足够大，大到可以检测出多个算法之间的不同。例如，分类器A的分类精度是90.0%，分类器B的精度为90.1%，开发集有100个样例，则开发集不能检测出这0.1%的不同（将开发集输入分类器A、B进行分类，A、B的结果都是90个样例的类标签正确，所以不能区分A、B）。就我所见过的机器学习问题来说，100个样例的开发集确实小了点儿。一般，开发集都是1000到10000这么大。当你有了10000个样例时，你就能检测出那0.1的提高啦(注释1)（我觉得0.01也能检测出来）。对于成熟而重要的应用，例如广告，网页搜索，产品推荐，我见过一些团队为哪怕提高0.01%的精确度而努力工作，因为这对公司的利益有直接的影响。在这种情况下，开发集可以远大于10000，以便检测出更小的提高。测试集的大小呢？测试集应该大到在测试你的系统的整体性能时，是可信服的。一个比较流行的方法是取30%的数据作为测试集，这种方法在适量数据(100到10000个样例)时，效果很好。但在大数据时期，有的机器学习问题的样例甚至超过了十亿，此时，开发/训练集占总数据的比例减少了，但数量变多了。其实没有必要具有过大的开发/测试集，只需要能评估你的算法就行了。 [注释1]： 理论上，还可以测试算法的变化是否引起统计显著性差异。实际中，多数团队都不做这个测试（除非他们要发表学术性研究论文），我发现统计显著性检验对临时进展（interim progress，即每次改变算法获得的改进）没有什么用","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 6.Your dev and test sets should come from the same distribution","date":"2017-01-28T05:26:41.000Z","path":"2017/01/28/MLY-6-Your-dev-and-test-sets-should-come-from-the-same-distribution/","text":"根据你app应用的市场：（i）US，（ii）China，（iii）India，（iv）Other，你将猫图片数据分为四个部分。若想要一个开发集和一个测试集时，我们可以随机地选两个作为开发集，剩下的两个作为测试集。例如，可以选US和India作为开发集，China和Other作为测试集。一旦确定好了开发集和测试集，你的小团队将专注于提高学习算法在开发集上的表现。因此，开发集应该反映你想要改进的任务：在四个地理位置上表现都好，而不是两个。此外，当开发集和测试集分布不同时，还会有问题：有可能你的团队将要构建一个在开发集上表现很好的东西，却发现这个东西在测试集上表现的很差。这种结果有多挫败和浪费精力，避免让它发生在你身上吧。举个例子：假设你的团队做出的系统在开发集上表现很好，而在测试集上却表现不佳。如果你的开发集和测试集来自于用一个分布，那么你会知道是过拟合开发集(是否还有可能是训练集过拟合？)了，此时明显的解决办法就是获得更多开发集数据。但是，若开发集和测试集来自不同的分布，那么就不容易找到解决方向了。有几个导致在测试集上表现不佳的原因： 过拟合开发集 测试集比开发集更严格。此时你的算法可能跟预期的一样好了，没有能改进很多的方法了 测试集不一定更严格，只是和开发集不同而已。所以在开发集上表现好的算法不一定在测试集上表现的也好。对于这种情况，你为提高算法在开发集上的表现所做的工作，就没用了。 在“机器学习应用”上工作已经够难了，开发集和测试集不匹配又将会带来不确定性：若算法在开发集上表现提高，是否在测试集上表现也会提高？开发集和测试集的不匹配让我们更难找出什么样的解决方向能奏效，从而使得很难确定工作的优先级。如果你正在处理 3rd party benchmark 问题，这个问题的创建者可能已经指定开发集和测试集来自不同的分布。在这种情况下，运气，而非技术，将会对算法表现有较大影响。如何将“在一个分布上训练的算法能泛化到其他分布上”是一个重要的科研问题。但是，如果你的目标是在一个特定的机器学习应用领域取得进步，而不是使科研问题进步，那么我建议你选择开发集和测试集来自同一分布，这会使你的团队更有效率。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 5.your development and test sets","date":"2017-01-28T02:52:40.000Z","path":"2017/01/28/MLY-5-your-development-and-test-sets/","text":"让我们回想一下猫图片那个例子：你们公司运行一个手机app，用户往app上上传许多不同物体的图片，而你想从这些图片中让机器自动找到猫图片。你的团队从不同的网站上获得了包含猫的图片（正样例）和不包含猫的图片（负样例）。然后将70%的图片用作训练集，30%的图片用作测试集。使用这些图片，你的团队构建了一个猫检测器，这个检测器在训练集和测试集上表现很好。但是，当你将这个分类器发布到移动端app时，其表现却很差。为什么会这样？你发现用户上传的图片与组成训练集的图片有不同的外观：用户上传的图片是用手机拍摄的，因此图片分辨率较低，模糊，并且光线不太理想。因你的训练/测试集都是由网站上的图片组成的，所以由训练集训练除的算法不能很好地泛化到手机图片的分布。在大数据时代之前，在机器学习中，随机70%/30%分割数据集为训练集和测试集是一个常见规则。这种规则可行，但是，在越来越多的“训练时的分布（例如上例网站图片）和最终关心的问题的分布（上例的手机图片）是不同的”应用上， 这种规则却不是一个好主意。我们通常定义： 训练集 – 用来训练你的学习算法 开发集(development set) – 用来调整参数，选择特征，根据学习算法做出其他决定。有时也叫hold-out交叉验证集。 测试集 – 用来评价学习算法的表现，不能将其用于对学习算法和算法参数的选择。 一旦你们定义里开发集和测试集，你的团队就可以尝试不同的ideas（例如，不同的学习算法参数），从而发现哪种ideas表现最好。开发集和测试集能使你的团队快速地看到你们的学习算法表现的怎么样。换句话来说，开发集和测试集能指引你的团队对机器学习系统做出最重要的改变。所以，你应该做到： 选择的开发集和测试集能够反映你期望将来能得到的并且算法在上面表现不错的数据的特点。换句话说，你的测试集不应该只是可利用的数据的30%，特别是将来的数据（移动端app图像）和你的训练数据（网站图像）不同时。如果你还没有发布app，没有用户的你可能不能得到能够精确反映算法将来处理的数据的数据集。但是，你可以尝试得到一些近似数据。例如，拉上你的亲朋好友，让他们用手机拍照片并传给你。一旦你的app发布了，你就能够根据用户的数据更新你的开发/测试数据集啦！假如，你没有任何途径能够得到一些近似数据，你也可以使用网站图像先开始。但你应该意识到，这会导致系统的泛化能力不好。我们需要判断一下，在构建很棒的开发集和测试集上应该投资多少。但是不要妄想训练时的分布和测试的分布相同。尝试挑选能够反映“算法最终能在其上表现好的数据”的样例，而不是你碰巧得到的什么数据（这里的for training应该理解成整个训练过程，包括训练，交叉验证和测试）。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- 4.Scale drivers machine learning progress","date":"2017-01-27T02:39:40.000Z","path":"2017/01/27/MLY-Scale-drivers-machine-learning-progress/","text":"深度学习（神经网络）的很多想法已经存在几十年了，为什么这些想法现在才流行起来？促使机器学习进来的进步的两大因素是： 可得到的数据。现在，人们会花费更多的时间在数字设备上（例如笔记本，手机）。人们的数字行为产生了大量的数据，而这些数据可以用来训练我们的学习算法。 计算的规模。 也就是在近几年，我们才有计算能力训练足够大的神经网络，这种足够大的网络能利用我们拥有的巨大的数据集。 具体来说，即使你积累了更多数据，把这些数据用于训练旧的学习算法（例如逻辑回归），这些算法仍然表现平平。即旧的学习算法的学习曲线是“平坦”的，即使你给它更多数据，它仍然停止增长。这种情况就像旧算法不知道如何处理我们现在拥有的数据似的。对于同样的监督学习任务，如果我们训练一个小的神经网络，将会得到一个稍微好点的结果：这里，小的神经网络是指神经网络的隐藏单元/层/参数较小。如果你训练越来越大的神经网络，你将会得到越来越好的结果[注释1]。因此，当有（i）训练一个非常大的网络（训练过程会遵循上图中的绿色曲线）；(ii)拥有大规模数据 这两个条件满足时，你将得到最好的表现。还有一些其他的细节，例如神经网路的结构也很重要，并且在这个领域中有很多创新工作的提出。但目前来说，提高网络表现性能的相对可靠地方法就是(i)训练更大的网络(ii)收集更多数据。满足条件（i）和（ii）的过程是相当复杂的，本书将详细讨论如何完成这项工作的细节。首先，我们将从既能用于传统学习算法又能用于神经网络的通用策略开始，然后讲述构建深度学习系统的最现代的策略。[注释1]: 下图表示神经网络在小规模数据中表现挺好。但这种好的程度，不如神经网络在大规模数据中的表现好的程度（可以看出随着数据规模增大，Large NN倾斜越大）。在小规模数据中，传统算法表现的好与不好取决于如何手工设计特征。例如，如果你有20个训练样例，选择逻辑回归还是神经网络并不重要，手工设计特征将比选择算法对结果影响更大。但是如果有100万样例，还是选择神经网络把。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- 3.Prerequisites and Notation","date":"2017-01-27T01:47:11.000Z","path":"2017/01/27/MLY-Prerequisites-and-Notation/","text":"如果你学习过机器学习课程，或者有使用监督学习的经验，你将能够更好理解这本书。本书中，我在讲述下面的章节时，会假设你对监督学习很熟悉。监督学习是在有标签的数据(x,y)中学习从x映射到y的一个函数，线性回归、逻辑回归、神经网络都是监督学习算法。目前存在很多机器学习的形式，但机器学习产生的实际价值多数都是监督学习带来的。在这本书中，我将会多次提到神经网络（即深度学习）。为学习这本书，你需要对神经网络（或深度学习）有一个基本的理解。如果你对神经网络（或深度学习）不理解，你可以看Coursera上的机器学习的前三个星期的课程，网址是http://ml-class.org","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- 2.How to use this book to help your team?","date":"2017-01-25T13:47:08.000Z","path":"2017/01/25/Machine-Learning-Yearning-How-to-use-this-book-to-help-your-team/","text":"在读完这本书后，你将会对“如何为机器学习项目设定技术方向”有一个深入的理解。但是你的队友可能不理解你为什么偏向那个技术方向，例如你想让你队友们定义一个单数评价矩阵，但是他们对此不信服。你将会如何说服他们？这就是我将章节设置得如此短的原因：你可以将1-2页的内容打印出来，让你的队友读一读。对事务优先级的改变，将对你的团队的工作效率有很大的影响。多提几个对团队有帮助的改变策略，你就会成为众人眼中的超人啦！","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- 1.Why Machine Learning Strategy?","date":"2017-01-25T13:42:47.000Z","path":"2017/01/25/Machine-Learning-Yearning-Why-Machine-Learning-Strategy/","text":"机器学习是很多应用的基础，如网页搜索，反垃圾邮件，语音识别，推荐系统等。如果你的团队正在做一个基于机器学习的应用，这本书能够帮你的团队取得快速的进步。 一个例子：提供猫图片的创业公司假如你现在正在创建一个为猫奴提供很多猫图片的公司，你用神经网络建立一个从图片中检测猫的计算机视觉系统。但悲剧的是，你发现你的学习算法精度不够，面临提高猫检测器精度的巨大压力，你要怎么做？你的小团队很给力，他们提出以下解决问题的方向： 收集更多数据：收集更多猫的图片 收集多样化训练数据：例如，收集具有不同姿势的猫图片；收集具有不同毛色的猫图片；收集拍摄时相机参数设置不同的图片 训练时间长一点儿：将梯度下降法的循环次数设置多一点儿 尝试更大的神经网络，所谓更大是指层数更多或者说隐藏单元更多或者说权重参数更多。 尝试更小的神经网络 尝试添加正则化项（例如L2正则） 改变神经网络的结构（例如改变激活函数，隐藏单元等） …… 在以上所述的方向中，如果能你能选择一个正确的方向，你将会建立一个领先于他人的猫图片平台，你的创业公司也会成功。若你选择一个不好的方向，你可能会浪费几个月。你将如何处理？这本书将会告诉你如何处理。对于多数机器学习问题，我们都能找到一些线索，这些线索能够告诉我们什么是有效的尝试，什么是无效的尝试。学会解读这些线索，将可能节约你数月甚至数年的开发时间。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- Machine Learning Yearning 目录","date":"2017-01-25T13:11:50.000Z","path":"2017/01/25/this-is-meng/","text":"寒假，玩心太重，无法静下心来看一篇论文。翻译一下Machine Learning Yearning，期待能坚持下去。 为何需要机器学习策略？ 如何用这本书帮助你的团队？ 预备知识和符号约定 规模（scale）促使机器学习进步","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]}]