[{"title":"什么是metric learning","date":"2017-03-21T06:04:27.410Z","path":"2017/03/21/什么是metric-learning/","text":"参考：http://blog.csdn.net/Nehemiah_Li/article/details/44230053?locationNum=3 度量（Metric）的定义 : 在数学中，一个度量（或距离函数）是一个定义集合中元素之间距离的函数。一个具有度量的集合被称为度量空间。 为什么要用度量学习？很多的算法越来越依赖于在输入空间给定的好的度量。例如K-means、K近邻方法、SVM等算法需要给定好的度量来反映数据间存在的一些重要关系。这一问题在无监督的方法（如聚类）中尤为明显。举一个实际的例子，考虑图1的问题，假设我们需要计算这些图像之间的相似度（或距离，下同）（例如用于聚类或近邻分类）。面临的一个基本的问题是如何获取图像之间的相似度，例如如果我们的目标是识别人脸，那么就需要构建一个距离函数去强化合适的特征（如发色，脸型等）；而如果我们的目标是识别姿势，那么就需要构建一个捕获姿势相似度的距离函数。为了处理各种各样的特征相似度，我们可以在特定的任务通过选择合适的特征并手动构建距离函数。然而这种方法会需要很大的人工投入，也可能对数据的改变非常不鲁棒。度量学习作为一个理想的替代，可以根据不同的任务来自主学习出针对某个特定任务的度量距离函数。softmax使用欧氏距离，会把同类图片都聚到一个点中，针对又是马又是人的图片，它只能框出一类knn最合适的是学习马氏距离，怎么学？要给出先验知识，哪两个数据更相似，欧式距离不可靠。SVM也是metric learning的一种，因为kernel matrix就是相似度","tags":[]},{"title":"MLY -- 14.Evaluating multiple ideas in parallel during error analysis","date":"2017-02-18T05:08:00.000Z","path":"2017/02/18/MLY-14-Evaluating-multiple-ideas-in-parallel-during-error-analysisi/","text":"你的团队有提高猫检测器的几个点子： 解决狗被分类为猫的问题 解决“大猫”（狮子，豹等）被认为是家猫（宠物）的问题 提高系统在模糊图像上的性能 …… 你可以并行地评估所有这些点子。我通常创建一个电子表格，并在查看这100张误分类开发集图片时填写这张表格，并记下有助于我记起具体是哪个例子的评论。下面用四个图片来演示我是怎么做的： image dog great cat blurry comments 1 &radic; 不平常的斗牛犬颜色 2 &radic; 3 &radic; &radic; 狮子;雨天在动物园拍摄的照片 4 &radic; 树后面的豹 % of total 25% 50% 50% 上面的图片3既是大猫又是模糊图片：一个例子可以属于多种类别。这就是为什么表格底部的百分数加起来不等于100%的原因。虽然在上述描述过程中，我首先确定类别（狗，大猫，模糊图片），然后将误分类图片分到每个类别中，在实际中，一旦你开始查看那些抽取的误分类图片，你可能会被启发从而提出新的类别。例如，你在查看了十几个图片后，意识到很多误分类图片是被Instagram过滤器预处理过后的图片。此时，你就可以在电子表格上加上Instagram一栏了。人工查看误分类图片，并在查看时问问自己如何/是否能够给出这些图片的正确标签，将能启发你提出新的错误类别和解决方案。最有用的错误类别是针对它你已有了提升方案的类别。例如，Instagram类别将是最有用的，如果你有了一个“撤销”Instagram过滤器从而将图片恢复到原始图片的方法。但你不必纠结于已经有了改进想法的错误类别；错误分析阶段的目标是建立你关于“哪个领域是最有前途的、最值得关注的”的直觉。错误分析是一个迭代过程。你可以从没有任何类别开始。通过查看图像，你可能会想出一些关于错误类别的ideas。然后，在对一些图片手工分类后，你可能会受到启发并提出新的类别，然后返回按照新类别重新检查图片，重复此循环。假设你完成了100个误分类开发集图片的错误分析，并得到： image dog great cat blurry comments 1 &radic; 不平常的斗牛犬颜色 2 &radic; 3 &radic; &amp;radic 狮子；雨天在动物园拍摄的图片 4 &radic; 树后面的豹 … … … … … % of total 8% 43% 61% 你现在知道了强调消除狗狗错误的项目最多只能消除8%的错误，致力于消除大猫和模糊图片的错误能够提高更多。你可以致力于后两个类别之一。如果你的团队有足够的人，可同时追求多个方向，你可以要求一些工程师致力于大猫和模糊图片两个类别。错误分析不会产生一个刚性(rigid)的数学公式，告诉你哪个任务应该是优先级最高的。你还必须考虑你希望在不同类别上取得多少进展，以及处理每个类别所需的工作量。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 13.Error analysis:look at dev set examples to evaluate ideas","date":"2017-02-17T12:48:15.000Z","path":"2017/02/17/MLY-13-Error-analysis-look-at-dev-set-examples-to-evaluate-ideas/","text":"当你在玩你的猫app时，你发现几个把狗狗分类成猫的例子。但是有些狗狗真的很像猫！你的团队成员提出结合第三方软件将会使系统在处理狗狗图片上更好。但这将会花费一个月，团队成员对此很是热情。你应该要求他们做吗？在投入一个月在这个项目上之前，我建议你首先评估一下这样做后系统的准确率会提高多少。然后你能更理性地决定这样做是否值得，或者你最好把时间用在其他任务上。具体来说，你可以这样做：1.从你的系统错误分类的开发集中抽取100个，换句话说，就是系统犯错的典型例子。2.人工查看这些图片，看看这些图片中狗狗图片占的比例。观察被误分类图片例子的过程称为“错误分析”。在本例中，如果你发现只有5%的误分类图片为狗狗图片，那么无论你在狗狗问题上做了多少提升，你都不会摆脱大于5%的错误（因为这5%确实是狗狗图片）。也就是说，5%是第三方软件能帮助你的系统的“天花板”。因此，如果你下载的系统的准确率是90%（10%的错误率），那么使用第三方软件后，最好结果就是90.5%的准确率（或者说9.5%的错误率，105%=0.5%）。作为对比，如果你发现50张错误分类图片都是狗狗图片，那么使用第三方软件可能会对你的系统产生较大的影响，可以使系统从90%的准确率提升到95%（1050%）。这种对错误的简单计数过程可以使你估计使用第三方软件能产生的价值，它提供了定量依据，以此决定是否作出这笔投资。错误分析能帮你弄清不同的方向具有的潜在价值。我曾见过很多工程师都不愿进行错误分析。投入并实现ideas是一件很令人兴奋的事，但质疑idea是否值得投入时间却不是那么令人兴奋。但你不去质疑，可能导致你的团队花费一个月的时间才意识到，这个idea只能产生一点点好处。人工检测100张图片并不会花费很多时间。即使一分钟看一张，两个小时你也能看完。用两个小时，就能节约一个月的无用功，多合算！“错误分析”是指检查算法误分类图片样本的过程，以此来理解错误的深层原因。这既可以使你为应进行的步骤进行优先级的排序（如本例中，是组合第三方软件还是选择其他idea），又能激发新的方向（下一章我们会谈到）。接下来的几章还将介绍进行误差分析的最佳实践。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 12.Takeways:Setting up development and test sets","date":"2017-02-17T09:07:57.000Z","path":"2017/02/17/MLY-12-Takeways-Setting-up-development-and-test-sets/","text":"选择能反映“你未来期望得到并且希望算法在其上能表现得好的数据”的分布的开发集和测试集。这可能和你训练集的分布不同。 尽量使开发集和测试集的分布相同 为你的团队选择优化单数值评价度量。如果有多个目标，可以考虑将他们组合成单个公式（例如对多个错误度量（error metrics）取平均），或者定义满意度量和优化度量。 机器学习是个高迭代过程：在发现能使你满意的点子之前，你可能需要尝试很多点子。 拥有开发/测试集和单数值评价度量能帮助你评估算法，从而使迭代更快。 当开始一个全新的应用时，尽量尽快建立开发/测试集和评价度量，尽量少于一周。在成熟的应用上，可以花费时间长点儿。 以前启发式的训练/测试集按70%/30%分割的策略在数据量很大时就不适用了。开发和测试集可以少于30%。 你的开发集应该大到可以检测出算法准确率的有意义的改变，但没有必要太大。你的测试集应该大到对最终的算法的表现有个可信服的评估。 如果你的开发集和度量不再能指引正确的方向，你应快速改变它们：（i）如果过拟合了开发集，应获得更多的开发集数据。（ii）如果实际分布和开发/测试集的分布不同，获得新的开发/测试集。(iii)如果度量不再能测量对你来说重要的东西，改变度量。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 11.When to change dev/test sets and metrics","date":"2017-02-17T04:29:01.000Z","path":"2017/02/17/MLY-11-When-to-change-dev-test-sets-and-metrics/","text":"当开始做一个新项目时，我会快速选择开发集和测试集，因为这会给团队一个明确的目标。通常，我会要求我的团队在一周之内提出初始的开发集、测试集和度量，大多时候都不会多于一周。先提出一些不完美的东西使项目能前进下去，比过多考虑开发集、测试集、度量好的多。但是，一周时间线并不适用于成熟的应用。例如，垃圾邮件是一个成熟的深度学习应用。我曾见到在已经很成熟的系统上工作的团队花费数月的时间去获得好点儿的开发/测试集。如果在后续开发中，你发现初始开发集、测试集、度量偏离了目标方向（missed the mark），一定要快速地改进。例如，如果你的开发集+度量使分类器A得分优于分类器B，但是你的团队认为分类器B实际上对你的产品更好，那么这就可能是你需要改进你的开发集、测试集或者度量的时候了。1.实际的分布和开发集、测试集的分布不同假设你初始的开发/测试集主要由成年猫的图片组成。当发布了app后，出乎意料，你发现用户会上传很多幼年猫的图片。此时，开发/测试集的分布就不能代表实际的分布了。在这种情况下，你需要更新开发/测试集，使其更具代表性。2.过拟合开发集了重复地用开发集来评价ideas可能导致过拟合开发集。当项目开发结束时，需要在测试集上评估你的系统。如果你发现开发集的表现比测试集好，那么这可能就是过拟合开发集了。在这种情况下，你需要一个新的开发集（a fresh dev set）。如果你需要跟踪团队的进展，你可以用测试集定期评估你的系统（例如每周一次或每月一次）。但是不要用测试集做任何关于算法的决策，包括是否要回滚到上一个系统。如果你这么做了，系统将会开始过拟合测试集。并且不能再依赖测试集给出系统性能的完全无偏评估（当你发表论文或使用这个评估去做商业决策时就需要完全无偏评估）。3.度量（the metric）中不包括项目需要优化的东西假设对于猫分类器，你选择了分类准确度（classification accuracy）作为度量（metric），在这种度量下，分类器A比分类器B好。但是，当你实际去用这两种算法时，你发现分类器A偶尔会允许色情图片溜过去。即使分类器A准确率更高，但偶尔的一张色情图片会给用户留下坏印象，因此分类器A并不可取。此时，度量未能分辨出算法B实际上比算法A好。因此，度量不再可信，是时候改变度量了。例如，你可以改变度量使其惩罚色情图片通过。我强烈建议选择一个新的度量，并用这个新度量为团队定义一个新目标，而不是在没有可信度量的情况下工作太长时间甚至恢复到手动选择分类器。在项目期改变开发/测试集或评价度量是很常见的。初始的开发/测试集能帮助你快速进入迭代期。如果你发现开发/测试集或者度量不再能指引正确的方向，那也没事儿。只要改变它们并且保证你的团队知道新的方向就行了。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 10.Having a dev set and metric speeds up iterations","date":"2017-02-16T09:23:42.000Z","path":"2017/02/16/MLY-10-Having-a-dev-set-and-metric-speeds-up-iterations/","text":"很难事先知道什么算法最适合一个新问题。就算是经验丰富的机器学习研究人员通常也需要尝试一系列方法后才能发现一些令人满意的方法。当构建一个机器学习系统时，我经常：1.以一些如何构建系统的想法开始2.用代码实现这些想法3.做实验，实验会告诉我们哪些想法效果好。（通常，我开始的几个想法效果并不好）基于这些已学到的想法，我们回到第1步再产生更多想法，并循环下去。上图就是迭代过程。这一圈你走的越快，你的进展就越快。这就是为什么开发集和测试集重要：当你尝试一个想法时，你可以在开发集上测量想法的表现，这能使你快速地发现是否你正在朝着正确的方向前进。与此相对的是，假设你没有具体的开发集和度量（metric），每次你的团队开发一个新的猫分类器，就需要将这个分类器合并到你的app中，然后玩上几小时感觉一下新分类器是否有提升。这过程相当慢呀是不是？而且，当你的分类器从95.0%提升到95.1%时，你并不能通过玩app感觉出来这0.1%呀。拥有开发集和度量可以使你快速递检测出哪个想法会使分类集得到一点提升，从而使你快速判断出哪个想法需要再微调，哪个想法需要抛弃。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 9.Optimizing and satisficing metrics","date":"2017-02-16T07:22:04.000Z","path":"2017/02/16/MLY-9-Optimizing-and-satisficing-metrics/","text":"其实，还有一种组合多个评价度量的方法。假设，你将准确率和运行时间看做同等重要的评价项，你需要从以下三个分类器中选择一个： classifier accuracy running time A 90% 80ms B 92% 95ms C 95% 1500ms 如按照上一章的方法，通过将准确率和运行时间放入一个公式中得到单数值度量看起来是不自然的： Accuracy -0.5*RunningTime你可以这样做：首先，确定能接受的运行时间，例如运行时间在100ms内都是可接受的；然后，在满足运行时间这个条件下，最大化准确率。这里运行时间是一个“满意度量”——分类器只需要在“满意度量”下足够好，这意味着，你最多能花费100ms。准确率是“优化度量”。如果你想平衡（trade off）N个评价项，例如模型的二进制文件大小（这对手机app很重要，用户们不会喜欢下载文件大的app）、运行时间、准确率，你可能考虑设置N-1项评价为“满意度量”。换句话说，你只需要这N-1个评价度量们满足一个确定值。在这种情况下，你就可以定义最后一个评价为“优化度量”。例如，为二进制文件大小和运行时间设置一个阈值，然后在这些些约束条件下优化准确率。最后，再举一例。假设你正在构建一个硬件设备：当用户通过麦克风说一个“唤醒词”时，系统被唤醒。例如，Amazon Echo被“Alexa”唤醒，Apple Siri被“Hey Siri”唤醒；Android被“Okay Google”唤醒，Baidu apps被“Hello Baidu”唤醒。你关心的评价项有假正率（false positive rate）——没有人唤系统时系统却唤醒的频率，假负率（false negative rate ）—— 当有人唤系统时系统没被唤醒的频率。开发这个系统时应设置的合理目标是：在每24小时不超过一次假唤醒（false positive）的条件下（满意度量），最小化假负率（优化度量）。一旦你的团队明确了要优化的评价度量，他们将会进展更快。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 6.Your dev and test sets should come from the same distribution","date":"2017-01-28T05:26:41.000Z","path":"2017/01/28/MLY-6-Your-dev-and-test-sets-should-come-from-the-same-distribution/","text":"根据你app应用的市场：（i）US，（ii）China，（iii）India，（iv）Other，你将猫图片数据分为四个部分。若想要一个开发集和一个测试集时，我们可以随机地选两个作为开发集，剩下的两个作为测试集。例如，可以选US和India作为开发集，China和Other作为测试集。一旦确定好了开发集和测试集，你的小团队将专注于提高学习算法在开发集上的表现。因此，开发集应该反映你想要改进的任务：在四个地理位置上表现都好，而不是两个。此外，当开发集和测试集分布不同时，还会有问题：有可能你的团队将要构建一个在开发集上表现很好的东西，却发现这个东西在测试集上表现的很差。这种结果有多挫败和浪费精力，避免让它发生在你身上吧。举个例子：假设你的团队做出的系统在开发集上表现很好，而在测试集上却表现不佳。如果你的开发集和测试集来自于用一个分布，那么你会知道是过拟合开发集(是否还有可能是训练集过拟合？)了，此时明显的解决办法就是获得更多开发集数据。但是，若开发集和测试集来自不同的分布，那么就不容易找到解决方向了。有几个导致在测试集上表现不佳的原因： 过拟合开发集 测试集比开发集更严格。此时你的算法可能跟预期的一样好了，没有能改进很多的方法了 测试集不一定更严格，只是和开发集不同而已。所以在开发集上表现好的算法不一定在测试集上表现的也好。对于这种情况，你为提高算法在开发集上的表现所做的工作，就没用了。 在“机器学习应用”上工作已经够难了，开发集和测试集不匹配又将会带来不确定性：若算法在开发集上表现提高，是否在测试集上表现也会提高？开发集和测试集的不匹配让我们更难找出什么样的解决方向能奏效，从而使得很难确定工作的优先级。如果你正在处理 3rd party benchmark 问题，这个问题的创建者可能已经指定开发集和测试集来自不同的分布。在这种情况下，运气，而非技术，将会对算法表现有较大影响。如何将“在一个分布上训练的算法能泛化到其他分布上”是一个重要的科研问题。但是，如果你的目标是在一个特定的机器学习应用领域取得进步，而不是使科研问题进步，那么我建议你选择开发集和测试集来自同一分布，这会使你的团队更有效率。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]}]