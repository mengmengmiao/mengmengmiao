[{"title":"MLY翻译 -- Machine Learning Yearning 目录","date":"2027-01-25T13:11:50.000Z","path":"2027/01/25/this-is-meng/","text":"NG新出了一本书：Machine Learning Yearning，里面有各种指导机器学习落实到项目的建议，来自NG的经验，希望对做实验有帮助。 1.为何需要机器学习策略？ 2.如何用这本书帮助你的团队？ 3.预备知识和符号约定 4.规模（scale）促使机器学习进步 5.开发集和测试集 6.开发集和测试集需要来自同一分布 7.开发集和测试集需要多大？ 8.为你的团队选择一个单数值评价度量吧 9.优化度量和满意度量 10.有了开发集和度量，能加速迭代 11.何时改变开发集和度量？ 12.关键点：建立开发集和测试集的建议 13.错误分析：通过观察开发集来评价ideas 14.错误分析：同时考虑多个ideas","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"python tips 1","date":"2017-03-12T09:15:34.000Z","path":"2017/03/12/python-t-tips-1/","text":"1，a=[[1,2][3,4]] a[0,0] #错误 a[0][0] #正确 a=numpy.array([[1,2],[3,4]]) a[0,0] #正确 a[0][0] #正确2, a=numpy.array( [ [[1,2,21],[3,4,43]], [[5,6,65],[7,8,87]], [[9,10,19],[11,12,2111]] ]) a[:,:,(2,1,0)]是将第三维的元素的顺序按照2,1,0重新排列3，os.path.join(“home”, “me”, “mywork”) 将“home/me/mywork”返回4.将print输出打印到文件夹： import cPickle f = open(‘meng.txt’,’w+’) # f指向 存print输出结果的文件 s = cPickle.load(open(‘meng.pkl’)) # s是输出结果 print &gt;&gt; f,s5.sys.path —— 动态地改变Python搜索路径 如果python中导入的package或module不在环境变量PATH中，那么可以使用sys.path将要导入的package或module加入到PATH环境变量中。 import sys sys.path.append(’引用模块的地址’) 或者 import sys sys.path.insert(0, ‘引用模块的地址’) 程序向sys.path添加的目录只会在此程序的生命周期之内有效，其他所有的对sys.path的动态操作也是如此。6.","tags":[]},{"title":"py-faster-rcnn/tools/demo.py","date":"2017-03-12T07:45:42.000Z","path":"2017/03/12/py-faster-rcnn-demo-py/","text":"这个demo.py展示用训练好的VGG16在示例图片上的测试效果。1._init_paths.py文件，确保py-faster-rcnn/lib和py-fast_rcnn/caffe-fast_rcnn/python加入环境变量12345678910111213141516171819202122import _init_paths from fast_rcnn.config import cfg #这4个文件都是py-fast_rcnn/lib/fast_rcnn/和/utils目录下的文件from fast_rcnn.test import im_detectfrom fast_rcnn.nms_wrapper import nmsfrom utils.timer import Timerimport matplotlib.pyplot as pltimport numpy as npimport scipy.io as sio #io可读取matplot文件import caffe, os, sys, cv2import argparse #用于解析命令行参数和选项的标准模块CLASSES = ('__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor') #[]:列表，():元组，内容不可修改，&#123;&#125;：字典NETS = &#123;'vgg16': ('VGG16', 'VGG16_faster_rcnn_final.caffemodel'), 'zf': ('ZF', 'ZF_faster_rcnn_final.caffemodel')&#125; 2.标上置信度大于0.5的框，标上分类名和置信度，标上标题12345678910111213141516171819202122232425262728293031def vis_detections(im, class_name, dets, thresh=0.5): \"\"\"Draw detected bounding boxes.\"\"\" inds = np.where(dets[:, -1] &gt;= thresh)[0] #返回置信度大于0.5的框在dets中的下标 if len(inds) == 0: #如果没有框的置信度大于0.5 return im = im[:, :, (2, 1, 0)] fig, ax = plt.subplots(figsize=(12, 12)) ax.imshow(im, aspect='equal') for i in inds: bbox = dets[i, :4] score = dets[i, -1] ax.add_patch( plt.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1], fill=False, edgecolor='red', linewidth=3.5) ) ax.text(bbox[0], bbox[1] - 2, '&#123;:s&#125; &#123;:.3f&#125;'.format(class_name, score), bbox=dict(facecolor='blue', alpha=0.5), fontsize=14, color='white') ax.set_title(('&#123;&#125; detections with ' 'p(&#123;&#125; | box) &gt;= &#123;:.1f&#125;').format(class_name, class_name, thresh), fontsize=14) plt.axis('off') plt.tight_layout() plt.draw() 3.123456789101112131415161718192021222324252627def demo(net, image_name): \"\"\"Detect object classes in an image using pre-computed object proposals.\"\"\" # Load the demo image im_file = os.path.join(cfg.DATA_DIR, 'demo', image_name) im = cv2.imread(im_file) # Detect all object classes and regress object bounds timer = Timer() timer.tic() scores, boxes = im_detect(net, im) timer.toc() print ('Detection took &#123;:.3f&#125;s for ' '&#123;:d&#125; object proposals').format(timer.total_time, boxes.shape[0]) # Visualize detections for each class CONF_THRESH = 0.8 NMS_THRESH = 0.3 for cls_ind, cls in enumerate(CLASSES[1:]): cls_ind += 1 # because we skipped background cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)] cls_scores = scores[:, cls_ind] dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32) keep = nms(dets, NMS_THRESH) dets = dets[keep, :] vis_detections(im, cls, dets, thresh=CONF_THRESH) 4.解析./tools/demo.py [此处可以填参数] 传递过来的参数。1234567891011121314def parse_args(): \"\"\"Parse input arguments.\"\"\" parser = argparse.ArgumentParser(description='Faster R-CNN demo') parser.add_argument('--gpu', dest='gpu_id', help='GPU device id to use [0]', default=0, type=int) parser.add_argument('--cpu', dest='cpu_mode', help='Use CPU mode (overrides --gpu)', action='store_true') parser.add_argument('--net', dest='demo_net', help='Network to use [vgg16]', choices=NETS.keys(), default='vgg16') args = parser.parse_args() return args 5.对caffe的配置12345678910111213141516171819202122232425262728293031323334353637if __name__ == '__main__': cfg.TEST.HAS_RPN = True # Use RPN for proposals 修改配置文件py-faster-rcnn/lib/fast-rcnn/config.py 的__C.TEST.HAS_RPN = False args = parse_args() prototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0], 'faster_rcnn_alt_opt', 'faster_rcnn_test.pt') # 这个路径是~/py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt caffemodel = os.path.join(cfg.DATA_DIR, 'faster_rcnn_models', NETS[args.demo_net][1]) #这个路径是:~/py-faster-rcnn/data/faster_rcnn_models/VGG16_faster_rcnn_final.caffemodel if not os.path.isfile(caffemodel): raise IOError(('&#123;:s&#125; not found.\\nDid you run ./data/script/' 'fetch_faster_rcnn_models.sh?').format(caffemodel)) #出现这个错误是因为你没有下载训练好的模型 if args.cpu_mode: caffe.set_mode_cpu() #设置为cpu模式 在caffe-fast-rcnn/matlab/+caffe/set_mode_cpu.m ？ else: caffe.set_mode_gpu() caffe.set_device(args.gpu_id) cfg.GPU_ID = args.gpu_id net = caffe.Net(prototxt, caffemodel, caffe.TEST) # 网络结构，网络参数，测试 print '\\n\\nLoaded network &#123;:s&#125;'.format(caffemodel) # Warmup on a dummy image im = 128 * np.ones((300, 500, 3), dtype=np.uint8) for i in xrange(2): _, _= im_detect(net, im) # im_detect方法，返回rpn的score和框 &lt;font color =red&gt;这句看不懂&lt;/font&gt; im_names = ['000456.jpg', '000542.jpg', '001150.jpg', '001763.jpg', '004545.jpg'] for im_name in im_names: print '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' print 'Demo for data/demo/&#123;&#125;'.format(im_name) demo(net, im_name) plt.show()","tags":[{"name":"faster-rcnn源码解读","slug":"faster-rcnn源码解读","permalink":"https://mengmengmiao.github.io/tags/faster-rcnn源码解读/"}]},{"title":"如何远程ssh运行图形化应用？putty+Xming","date":"2017-03-08T14:00:54.000Z","path":"2017/03/08/如何远程ssh运行图形化应用？putty-Xming/","text":"参考http://www.bubuko.com/infodetail-807886.htmlstep 1 ：先去搜Putty.exe装在windows上。step 2 : 搜Xming装在windows上。（到sourceforge上下载：http://sourceforge.net/projects/xming/?source=typ_redirect， 分别下载 xming 与 xming-fonts。） Xming-6-9-0-31-setup.exe Xming-fonts-7-5-0-11-setup.exe 装完Xming和Xming-fonts，点XLaunch，一路next，直到结束。step 3 ：Unix/Linux 服务器的配置 修改下要登录的服务器的配置文件，增加对 X11 Forwarding的支持： sudo vi /etc/ssh/sshd_config: （在文件的末尾，增加以下配置） X11Forwarding yesstep 4 ：Putty配置 session - &gt; Host Name填服务器IP，port填22，connection type选ssh，save session随便填个名字 - &gt; 点SSH下的X11 - &gt; 选择enable X11 forwarding ，X display location填localhost:0,MIT-Magic-Cookie-1填上 - &gt; 选Session，点save","tags":[]},{"title":"PSCP windows，linux传输文件/文件夹","date":"2017-03-08T09:53:38.000Z","path":"2017/03/08/PSCP-windows传输文件-文件夹到linux/","text":"step 1 :下载Putty，连上linux。下载PSCP.exe。下载step 2 : windows打开cmd，cd到PSCP.exe所在目录传文件step 3 : dos窗口输入PSCP C:/Users/admin/Desktop/meng.txt meng@1.1.1.1：，这样就拷贝meng.txt到你的linux用户主目录了传文件夹step 3 ：用putty连接linux ，在主目录下创建一个文件夹meng；打开windows的cmd，转到PSCP.exe所在目录，执行PSCP -r D:/meng meng@1.1.1.1:。这样文件夹就拷贝到linux用户主目录的meng文件夹里了。 从服务器下载文件：step 1：PSCP meng@1.1.1.1：/home/meng/py-faster-rcnn/meng.txt D:/meng （注意，不能写~/py-faster-rcnn/meng.txt）","tags":[]},{"title":"海洋遥感论文总结","date":"2017-03-08T04:44:33.000Z","path":"2017/03/08/“海洋遥感论文总结/","text":"关键字：海洋遥感，光学遥感(optical remote sensing)，目标识别(object recognition),目标检测（object detection）海洋遥感相比与陆地遥感、气象遥感难度更大，在全球气候变化、大洋环流、赤潮检测等领域具有重要作用。 1.Ship Detection in Spaceborne Optical Image with SVD Networks 期刊：IEEE Transactions on Geoscience &amp; Remote Sensing，2016 目标：ship；spaceborne optical image；数据来源于委内瑞拉遥感卫星（VRSS-1）和GaoFen-1（GF-1）卫星的图像，作者自己收集的数据，这部分数据用于训练和测试，另外从google地图选了600张ship图（加上从那两个卫星提取的ship图片）用于svm的训练； 算法：SVD Network，两阶段：第一阶段提取ship candidate，第二阶段对candidate用svm分类。 ps：这个文章说没有公共船舶检测数据集，没有人公布其源代码或软件。 2.Ship Detection in Images Obtained from the Unmanned Aerial Vehicle (UAV) 期刊：Indian Journal of Science and Technology 目标：ship；无人机拍摄视频；数据来源于Google地图，Yandex地图，一小部分来自互联网（共1000张）； 算法：使用一个形状级联和自动提取特征级联（在此部分使用DNN） ps：是将训练好的网络应用于识别无人机视频中的ship 3.Compressed-Domain Ship Detection on Spaceborne Optical Image Using Deep Neural Network and Extreme Learning Machine 期刊：IEEE Transactions on Geoscience and Remote Sensing，2015 目标：ship；spaceborne optical image；数据来源 4000张 5m分辨率的SPOT 5全色图片，大小为2000*2000，这个数据集是作者建立的； 算法：JPEG2000压缩域提取的小波系数进行候选船的提取，深度神经网络用来进行高层特征的提取和分类，极限学习机用来更有效低池化特征和做决策。 4.Rotation Sliding Window of the Hog Feature in Remote Sensing Images for Ship Detection 会议：2015 8th International Symposium on Computational Intelligence and Design 目标：ship；数据来源 404正样本，来自google地图，410负样本，来自high random I satellite data； 算法：Rotation Sliding Window+HOG+SVM 总结：在google上搜索了几天，发现在海洋遥感这一块，满足海洋遥感+光学图像两个条件的，识别ship的最多，而识别coastline/shoreline（海岸线移动），island，iceberg，海面油膜（远红外），海浪（预测风暴），海冰（有时会用到雷达图像），悬浮泥沙，海面污染，赤潮，叶绿素，海色等的非常少 5.基于光学遥感的海岛识别及算法研究 期刊：博士学位论文（浙江大学），2010 目标：海岛；光学图像；数据来源TM、ETM+、SPOT遥感数据，我觉得也是自己选取的实验数据 算法：几何精纠正、像元灰度重采样、二值化提取海岛图斑、Rs、Gis和DEM和数据的一体化合成等改进算法 6.基于SIFT-SVM的北冰洋海冰识别研究 期刊：图像与多媒体技术，2017 目标：海冰；SAR图像（实在找不到光学的）；论文中只说是一张图片分成198个子图像，我觉得也是自己选取的实验数据 算法：SIFT+K-menas+SVM（分割） 7.基于图像处理的海冰识别与追踪方法 期刊：硕士学位论文（大连理工大学）2014 目标：海冰；光学图像，EOS和普通拍摄图片；数据来源，作者收集 算法：分割算法 8.高分辨率可见光遥感图像港口及港内目标识别方法 期刊：硕士学位论文（中国科学技术大学），2005 目标：海冰；光学图像；Google Earth，QuickBird 算法：模仿人类利用记忆知识辨别事物的原理，利用对港口目标的先验信息的适当表述，采用SIFT 特征匹配的方法，判断并识别图像中的港口目标。 9.遥感图像中港口目标识别技术 期刊：南京航空航天大学学报，2008 目标：港口；没说用的什么图像，看着像光学；25张图像，作者自己收集 算法：分割 10.遥感图像中港口目标的检测与识别 期刊：硕士学位论文（哈尔滨工业大学），2008 目标：港口；光学图像；作者自己收集 算法： ps：我觉得一旦用于港口这种复杂目标的识别图像，都是光学的，光学的含有的纹理更多。 11.大尺度遥感图像中港口目标快速识别 期刊：模式识别与人工智能 目标：港口；光学；作者自己收集，18块10000*10000的1m分辨率的图像 算法：海陆分割，图像分块；EM算法、阂值分割方法、快速提取目标候选区域方法 总结：在ship、海岛的识别上，和我现在做的识别流程很像，但海冰、港口的识别，多用分割算法以及其他一些算法，港口内目标的识别则在加上港口这一先验条件下，再去识别，感觉此时的识别和我现在做的识别有点像。而海岸线的提取，则用一些其他的方法，轮廓提取。做海浪识别的我没搜到，可能对海浪的强度进行研究更有意义。赤潮也是，单纯的识别赤潮应该是无意义的，识别出面积什么的更有意义吧。","tags":[]},{"title":"安装faster rcnn","date":"2017-03-07T12:31:48.000Z","path":"2017/03/07/安装faster-rcnn/","text":"代码：https://github.com/rbgirshick/fast-rcnn Installation (sufficient for the demo)0.可能需要Python安装包：cython，python-opencv，easydict 先装一个python包管理器pip： sudo apt-get install python-pip 再安装那三个包 sudo pip install cython sudo apt-get install python-opencv (进入python，import cv2可看到是否安装成功) sudo pip install easydict1.从git clone faster r-cnn git clone –recursive https://github.com/rbgirshick/fast-rcnn.git2.cd 到py-faster-rcnn/lib （rbg的第二步不用管，本文中，依然使用py-faster-rcnn而没用FRCN_ROOT） make3.Download pre-computed Faster R-CNN detectors（rbg的那个步骤4，在已经安装了caffe的情况下，不用再做） cd $FRCN_ROOT ./data/scripts/fetch_faster_rcnn_models.sh Demo #After successfully completing basic installation, you’ll be ready to run the demo. cd $FRCN_ROOT ./tools/demo.py 出现错误： import error no module named _caffe。（在caffe-master下的py-faster-rcnn下执行./tools/demo.py,而不是和caffe-master并列的那个） 解决方法： #回到caffe-master make distribute #make dir for custom python modules, install caffe mkdir ~/python mv distribute/python/caffe ~/python #set PYTHONPATH (this should go in your .bashrc or whatever #回到用户主目录， export PYTHONPATH = $PYTHONPATH:~/caffe-master/python/caffe source ~/.bashrc 出现错误： importERROR no module named skimage.io 解决方法： #直接使用终端安装： #回到用户主目录 pip install -U scikit-image (过程漫长) 出现错误： imortERROR：no module named google.protobuf.internal 解决方法： sudo apt-get install python-protobuf 出现错误： ImportERROR: no module named gpu_nms 解决方法： 在py-faster-rcnn的lib中make 出现错误： ImportERROR：no module named _tkinter,please install the python-tk package 解决方法： sudo apt-get install python-tk 出现错误： Check failed:ReadProtorFromTextFile(param_file,param) failed to parse NetParameter file: / home/meng/caffe-master/py-faster-rcn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt ***Check failure stack trace*** 这个问题难解决，主要是因为py-faster-rcnn用的caffe是旧版本，而我的电脑上装的是新版本的，所以出现了这么多问题。所以做到这里，我决定把py-faster-rcnn的旧版本文件用新的替换下来（新文件来自caffe新版） 1.编译/py-faster-rcnn/caffe-fast-rcnn cd py-faster-rcnn/caffe-fast-rcnn cp Makefile.config.example Makefile.config 更改Makefile.config文件： #In your Makefile.config, make sure to have this line uncommented WITH_PYTHON_LAYER := 1 #Unrelatedly, it’s also recommended that you use CUDNN USE_CUDNN := 1 #配置一些引用文件（增加部分主要是解决新版本下，HDF5的路径问题） INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/lib/x86_64-linux-gnu/hdf5/serial/include LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial #启用Intel Parallel Studio XE 2016 BLAS := mkl #配置路径，实现caffe对Python接口的支持 PYTHON_LIB := /usr/local/lib #启用OpenCV 3.0, 去掉”#” OPENCV_VERSION =3 进行编译： make -j8 &amp;&amp; make pycaffe 因为这个版本所用的cudnn为旧版本的，可能与新环境的cudnn不兼容，导致出现如下错误： make: * [.build_release/src/caffe/util/db.o] Error 1 (可能是这个，类似吧，有点乱。) 解决办法： 1).将/py-faster-rcnn/caffe-fast-rcnn/include/caffe/util/cudnn.hpp 换成最新版的caffe里的cudnn的实现，即相应的cudnn.hpp. 2).将/py-faster-rcnn/caffe-fast-rcnn/src/caffe/layer里的，所有以cudnn开头的文件，例如cudnn_lrn_layer.cu，cudnn_pooling_layer.cpp，cudnn_sigmoid_layer.cu。 都替换成最新版的caffe里的相应的同名文件。 再次make -j8 &amp;&amp; make pycaffe 会遇到错误： make: * [.build_release/src/caffe/layers/cudnn_relu_layer.o] 错误 1 解决方法： 将/py-faster-rcnn/caffe-fast-rcnn/include/caffe/layers的，所有以cudnn开头的文件，例如cudnn_conv_layer.hpp，cudnn_lcn_laye.hpp 都替换成最新版的caffe里的相应的同名文件 遇到错误： ImportError:no module named yaml 解决方法： sudo apt-get install python-yaml 遇到错误： “/home/meng/.local/lib/python2.7/site-packages/matplot file “/usr/lib/python2.7/lib-tk/Tkinter.py” self.tk = _tkinter.create(screenName,baseName,className,interative,wantobjexts.useTk,sync,use) _tkinter.TclError: no display name and no $DISPALY environment variable 解决方法： 这错误是因为用putty远程登陆导致的，那个窗口弹不出来。给windows装个Xming吧。 2.运行faster-rcnn里的demo cd py-faster-rcnn/tools ./tools/demo.py","tags":[]},{"title":"MLY -- 14.Evaluating multiple ideas in parallel during error analysis","date":"2017-02-18T05:08:00.000Z","path":"2017/02/18/MLY-14-Evaluating-multiple-ideas-in-parallel-during-error-analysisi/","text":"你的团队有提高猫检测器的几个点子： 解决狗被分类为猫的问题 解决“大猫”（狮子，豹等）被认为是家猫（宠物）的问题 提高系统在模糊图像上的性能 …… 你可以并行地评估所有这些点子。我通常创建一个电子表格，并在查看这100张误分类开发集图片时填写这张表格，并记下有助于我记起具体是哪个例子的评论。下面用四个图片来演示我是怎么做的： image dog great cat blurry comments 1 &radic; 不平常的斗牛犬颜色 2 &radic; 3 &radic; &radic; 狮子;雨天在动物园拍摄的照片 4 &radic; 树后面的豹 % of total 25% 50% 50% 上面的图片3既是大猫又是模糊图片：一个例子可以属于多种类别。这就是为什么表格底部的百分数加起来不等于100%的原因。虽然在上述描述过程中，我首先确定类别（狗，大猫，模糊图片），然后将误分类图片分到每个类别中，在实际中，一旦你开始查看那些抽取的误分类图片，你可能会被启发从而提出新的类别。例如，你在查看了十几个图片后，意识到很多误分类图片是被Instagram过滤器预处理过后的图片。此时，你就可以在电子表格上加上Instagram一栏了。人工查看误分类图片，并在查看时问问自己如何/是否能够给出这些图片的正确标签，将能启发你提出新的错误类别和解决方案。最有用的错误类别是针对它你已有了提升方案的类别。例如，Instagram类别将是最有用的，如果你有了一个“撤销”Instagram过滤器从而将图片恢复到原始图片的方法。但你不必纠结于已经有了改进想法的错误类别；错误分析阶段的目标是建立你关于“哪个领域是最有前途的、最值得关注的”的直觉。错误分析是一个迭代过程。你可以从没有任何类别开始。通过查看图像，你可能会想出一些关于错误类别的ideas。然后，在对一些图片手工分类后，你可能会受到启发并提出新的类别，然后返回按照新类别重新检查图片，重复此循环。假设你完成了100个误分类开发集图片的错误分析，并得到： image dog great cat blurry comments 1 &radic; 不平常的斗牛犬颜色 2 &radic; 3 &radic; &amp;radic 狮子；雨天在动物园拍摄的图片 4 &radic; 树后面的豹 … … … … … % of total 8% 43% 61% 你现在知道了强调消除狗狗错误的项目最多只能消除8%的错误，致力于消除大猫和模糊图片的错误能够提高更多。你可以致力于后两个类别之一。如果你的团队有足够的人，可同时追求多个方向，你可以要求一些工程师致力于大猫和模糊图片两个类别。错误分析不会产生一个刚性(rigid)的数学公式，告诉你哪个任务应该是优先级最高的。你还必须考虑你希望在不同类别上取得多少进展，以及处理每个类别所需的工作量。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 13.Error analysis:look at dev set examples to evaluate ideas","date":"2017-02-17T12:48:15.000Z","path":"2017/02/17/MLY-13-Error-analysis-look-at-dev-set-examples-to-evaluate-ideas/","text":"当你在玩你的猫app时，你发现几个把狗狗分类成猫的例子。但是有些狗狗真的很像猫！你的团队成员提出结合第三方软件将会使系统在处理狗狗图片上更好。但这将会花费一个月，团队成员对此很是热情。你应该要求他们做吗？在投入一个月在这个项目上之前，我建议你首先评估一下这样做后系统的准确率会提高多少。然后你能更理性地决定这样做是否值得，或者你最好把时间用在其他任务上。具体来说，你可以这样做：1.从你的系统错误分类的开发集中抽取100个，换句话说，就是系统犯错的典型例子。2.人工查看这些图片，看看这些图片中狗狗图片占的比例。观察被误分类图片例子的过程称为“错误分析”。在本例中，如果你发现只有5%的误分类图片为狗狗图片，那么无论你在狗狗问题上做了多少提升，你都不会摆脱大于5%的错误（因为这5%确实是狗狗图片）。也就是说，5%是第三方软件能帮助你的系统的“天花板”。因此，如果你下载的系统的准确率是90%（10%的错误率），那么使用第三方软件后，最好结果就是90.5%的准确率（或者说9.5%的错误率，105%=0.5%）。作为对比，如果你发现50张错误分类图片都是狗狗图片，那么使用第三方软件可能会对你的系统产生较大的影响，可以使系统从90%的准确率提升到95%（1050%）。这种对错误的简单计数过程可以使你估计使用第三方软件能产生的价值，它提供了定量依据，以此决定是否作出这笔投资。错误分析能帮你弄清不同的方向具有的潜在价值。我曾见过很多工程师都不愿进行错误分析。投入并实现ideas是一件很令人兴奋的事，但质疑idea是否值得投入时间却不是那么令人兴奋。但你不去质疑，可能导致你的团队花费一个月的时间才意识到，这个idea只能产生一点点好处。人工检测100张图片并不会花费很多时间。即使一分钟看一张，两个小时你也能看完。用两个小时，就能节约一个月的无用功，多合算！“错误分析”是指检查算法误分类图片样本的过程，以此来理解错误的深层原因。这既可以使你为应进行的步骤进行优先级的排序（如本例中，是组合第三方软件还是选择其他idea），又能激发新的方向（下一章我们会谈到）。接下来的几章还将介绍进行误差分析的最佳实践。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 12.Takeways:Setting up development and test sets","date":"2017-02-17T09:07:57.000Z","path":"2017/02/17/MLY-12-Takeways-Setting-up-development-and-test-sets/","text":"选择能反映“你未来期望得到并且希望算法在其上能表现得好的数据”的分布的开发集和测试集。这可能和你训练集的分布不同。 尽量使开发集和测试集的分布相同 为你的团队选择优化单数值评价度量。如果有多个目标，可以考虑将他们组合成单个公式（例如对多个错误度量（error metrics）取平均），或者定义满意度量和优化度量。 机器学习是个高迭代过程：在发现能使你满意的点子之前，你可能需要尝试很多点子。 拥有开发/测试集和单数值评价度量能帮助你评估算法，从而使迭代更快。 当开始一个全新的应用时，尽量尽快建立开发/测试集和评价度量，尽量少于一周。在成熟的应用上，可以花费时间长点儿。 以前启发式的训练/测试集按70%/30%分割的策略在数据量很大时就不适用了。开发和测试集可以少于30%。 你的开发集应该大到可以检测出算法准确率的有意义的改变，但没有必要太大。你的测试集应该大到对最终的算法的表现有个可信服的评估。 如果你的开发集和度量不再能指引正确的方向，你应快速改变它们：（i）如果过拟合了开发集，应获得更多的开发集数据。（ii）如果实际分布和开发/测试集的分布不同，获得新的开发/测试集。(iii)如果度量不再能测量对你来说重要的东西，改变度量。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 11.When to change dev/test sets and metrics","date":"2017-02-17T04:29:01.000Z","path":"2017/02/17/MLY-11-When-to-change-dev-test-sets-and-metrics/","text":"当开始做一个新项目时，我会快速选择开发集和测试集，因为这会给团队一个明确的目标。通常，我会要求我的团队在一周之内提出初始的开发集、测试集和度量，大多时候都不会多于一周。先提出一些不完美的东西使项目能前进下去，比过多考虑开发集、测试集、度量好的多。但是，一周时间线并不适用于成熟的应用。例如，垃圾邮件是一个成熟的深度学习应用。我曾见到在已经很成熟的系统上工作的团队花费数月的时间去获得好点儿的开发/测试集。如果在后续开发中，你发现初始开发集、测试集、度量偏离了目标方向（missed the mark），一定要快速地改进。例如，如果你的开发集+度量使分类器A得分优于分类器B，但是你的团队认为分类器B实际上对你的产品更好，那么这就可能是你需要改进你的开发集、测试集或者度量的时候了。1.实际的分布和开发集、测试集的分布不同假设你初始的开发/测试集主要由成年猫的图片组成。当发布了app后，出乎意料，你发现用户会上传很多幼年猫的图片。此时，开发/测试集的分布就不能代表实际的分布了。在这种情况下，你需要更新开发/测试集，使其更具代表性。2.过拟合开发集了重复地用开发集来评价ideas可能导致过拟合开发集。当项目开发结束时，需要在测试集上评估你的系统。如果你发现开发集的表现比测试集好，那么这可能就是过拟合开发集了。在这种情况下，你需要一个新的开发集（a fresh dev set）。如果你需要跟踪团队的进展，你可以用测试集定期评估你的系统（例如每周一次或每月一次）。但是不要用测试集做任何关于算法的决策，包括是否要回滚到上一个系统。如果你这么做了，系统将会开始过拟合测试集。并且不能再依赖测试集给出系统性能的完全无偏评估（当你发表论文或使用这个评估去做商业决策时就需要完全无偏评估）。3.度量（the metric）中不包括项目需要优化的东西假设对于猫分类器，你选择了分类准确度（classification accuracy）作为度量（metric），在这种度量下，分类器A比分类器B好。但是，当你实际去用这两种算法时，你发现分类器A偶尔会允许色情图片溜过去。即使分类器A准确率更高，但偶尔的一张色情图片会给用户留下坏印象，因此分类器A并不可取。此时，度量未能分辨出算法B实际上比算法A好。因此，度量不再可信，是时候改变度量了。例如，你可以改变度量使其惩罚色情图片通过。我强烈建议选择一个新的度量，并用这个新度量为团队定义一个新目标，而不是在没有可信度量的情况下工作太长时间甚至恢复到手动选择分类器。在项目期改变开发/测试集或评价度量是很常见的。初始的开发/测试集能帮助你快速进入迭代期。如果你发现开发/测试集或者度量不再能指引正确的方向，那也没事儿。只要改变它们并且保证你的团队知道新的方向就行了。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 10.Having a dev set and metric speeds up iterations","date":"2017-02-16T09:23:42.000Z","path":"2017/02/16/MLY-10-Having-a-dev-set-and-metric-speeds-up-iterations/","text":"很难事先知道什么算法最适合一个新问题。就算是经验丰富的机器学习研究人员通常也需要尝试一系列方法后才能发现一些令人满意的方法。当构建一个机器学习系统时，我经常：1.以一些如何构建系统的想法开始2.用代码实现这些想法3.做实验，实验会告诉我们哪些想法效果好。（通常，我开始的几个想法效果并不好）基于这些已学到的想法，我们回到第1步再产生更多想法，并循环下去。上图就是迭代过程。这一圈你走的越快，你的进展就越快。这就是为什么开发集和测试集重要：当你尝试一个想法时，你可以在开发集上测量想法的表现，这能使你快速地发现是否你正在朝着正确的方向前进。与此相对的是，假设你没有具体的开发集和度量（metric），每次你的团队开发一个新的猫分类器，就需要将这个分类器合并到你的app中，然后玩上几小时感觉一下新分类器是否有提升。这过程相当慢呀是不是？而且，当你的分类器从95.0%提升到95.1%时，你并不能通过玩app感觉出来这0.1%呀。拥有开发集和度量可以使你快速递检测出哪个想法会使分类集得到一点提升，从而使你快速判断出哪个想法需要再微调，哪个想法需要抛弃。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 9.Optimizing and satisficing metrics","date":"2017-02-16T07:22:04.000Z","path":"2017/02/16/MLY-9-Optimizing-and-satisficing-metrics/","text":"其实，还有一种组合多个评价度量的方法。假设，你将准确率和运行时间看做同等重要的评价项，你需要从以下三个分类器中选择一个： classifier accuracy running time A 90% 80ms B 92% 95ms C 95% 1500ms 如按照上一章的方法，通过将准确率和运行时间放入一个公式中得到单数值度量看起来是不自然的： Accuracy -0.5*RunningTime你可以这样做：首先，确定能接受的运行时间，例如运行时间在100ms内都是可接受的；然后，在满足运行时间这个条件下，最大化准确率。这里运行时间是一个“满意度量”——分类器只需要在“满意度量”下足够好，这意味着，你最多能花费100ms。准确率是“优化度量”。如果你想平衡（trade off）N个评价项，例如模型的二进制文件大小（这对手机app很重要，用户们不会喜欢下载文件大的app）、运行时间、准确率，你可能考虑设置N-1项评价为“满意度量”。换句话说，你只需要这N-1个评价度量们满足一个确定值。在这种情况下，你就可以定义最后一个评价为“优化度量”。例如，为二进制文件大小和运行时间设置一个阈值，然后在这些些约束条件下优化准确率。最后，再举一例。假设你正在构建一个硬件设备：当用户通过麦克风说一个“唤醒词”时，系统被唤醒。例如，Amazon Echo被“Alexa”唤醒，Apple Siri被“Hey Siri”唤醒；Android被“Okay Google”唤醒，Baidu apps被“Hello Baidu”唤醒。你关心的评价项有假正率（false positive rate）——没有人唤系统时系统却唤醒的频率，假负率（false negative rate ）—— 当有人唤系统时系统没被唤醒的频率。开发这个系统时应设置的合理目标是：在每24小时不超过一次假唤醒（false positive）的条件下（满意度量），最小化假负率（优化度量）。一旦你的团队明确了要优化的评价度量，他们将会进展更快。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 8.Establish a single-number evaluation metric for your team to optimize","date":"2017-02-16T05:08:08.000Z","path":"2017/02/16/MLY-8-Establish-a-single-number-evaluation-metric-for-your-team-to-optimize/","text":"分类准确率(classification accuracy)是一个单数值的评价度量：将开发集（或测试集）输入分类器分类，分类器返回“被分类器正确分类的数据占输入数据集的比例”。如果分类器A获得97%的准确率，分类器B获得90%的准确率费，根据评价度量，分类器A较好。与此不同的是，精度率和召回率（precision and recall）就不是一个单数值评价度量：它用两个数值去评价分类器。多数值评价度量使得比较分类算法时变得困难。假设你的分类算法表现如下： classifier precision recall A 95% 90% B 98% 85% 这里，并不能直接看出两个分类器中哪个更好。 在实际开发过程中，你的团队将用不同的算法框架、模型参数、特征选择等。而使用单数值评价度量（如准确率）能快速地对这些算法的优劣做一个排序，从而看出哪一个算法更好。 如果你真的关心精确率和召回率，我建议你使用一种标准的方法将两种评价数值糅合成一个。例如，可取精确率和召回率的平均值作为评价数值；还可以取“F1 score”，F1 score是计算平均值的一种改进的方法，比取平均值的表现好多了。 classifier precision recall F1 score A 95% 90% 92.4% B 98% 85% 91.0% 单数值评价度量能让你从众多算法中快速选出哪个算法最优，它对众多算法进行偏好排序，因而明确了进展的方向。 最后，假设你分别在四个主要市场（(i)US,(ii)China,(iii)India,(iv)其他）跟踪你的猫分类器准确率，因此你得到四个度量。对这四个度量取平均或加权平均值，你就会得到一个评价数值。取平均或加权平均是将多个度量变成一个的最常用的方法。 ##注释： 精确率和召回率：以猫图片分类器为例，精确率是指在分类器分类成猫的图片中，真的是猫图片的比例。召回率是指在开发集（或测试集）中的猫图片被分类器分类成猫图片的概率。在高准确率和高召回率间，经常需要权衡（tradeoff）。 F1 score： 如果你想了解更多，可以点击 https://en.wikipedia.org/wiki/F1_score。F1 score就是精确率和召回率的几何平均值，是这样计算的 2/((1/precision)+(1/recall))。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 7.How large do the dev/test sets need to be?","date":"2017-01-30T13:12:33.000Z","path":"2017/01/30/MLY-7-How-large-do-the-dev-test-sets-need-to-be/","text":"开发集应该足够大，大到可以检测出多个算法之间的不同。例如，分类器A的分类精度是90.0%，分类器B的精度为90.1%，开发集有100个样例，则开发集不能检测出这0.1%的不同（将开发集输入分类器A、B进行分类，A、B的结果都是90个样例的类标签正确，所以不能区分A、B）。就我所见过的机器学习问题来说，100个样例的开发集确实小了点儿。一般，开发集都是1000到10000这么大。当你有了10000个样例时，你就能检测出那0.1的提高啦(注释1)（我觉得0.01也能检测出来）。对于成熟而重要的应用，例如广告，网页搜索，产品推荐，我见过一些团队为哪怕提高0.01%的精确度而努力工作，因为这对公司的利益有直接的影响。在这种情况下，开发集可以远大于10000，以便检测出更小的提高。测试集的大小呢？测试集应该大到在测试你的系统的整体性能时，是可信服的。一个比较流行的方法是取30%的数据作为测试集，这种方法在适量数据(100到10000个样例)时，效果很好。但在大数据时期，有的机器学习问题的样例甚至超过了十亿，此时，开发/训练集占总数据的比例减少了，但数量变多了。其实没有必要具有过大的开发/测试集，只需要能评估你的算法就行了。 [注释1]： 理论上，还可以测试算法的变化是否引起统计显著性差异。实际中，多数团队都不做这个测试（除非他们要发表学术性研究论文），我发现统计显著性检验对临时进展（interim progress，即每次改变算法获得的改进）没有什么用","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 6.Your dev and test sets should come from the same distribution","date":"2017-01-28T05:26:41.000Z","path":"2017/01/28/MLY-6-Your-dev-and-test-sets-should-come-from-the-same-distribution/","text":"根据你app应用的市场：（i）US，（ii）China，（iii）India，（iv）Other，你将猫图片数据分为四个部分。若想要一个开发集和一个测试集时，我们可以随机地选两个作为开发集，剩下的两个作为测试集。例如，可以选US和India作为开发集，China和Other作为测试集。一旦确定好了开发集和测试集，你的小团队将专注于提高学习算法在开发集上的表现。因此，开发集应该反映你想要改进的任务：在四个地理位置上表现都好，而不是两个。此外，当开发集和测试集分布不同时，还会有问题：有可能你的团队将要构建一个在开发集上表现很好的东西，却发现这个东西在测试集上表现的很差。这种结果有多挫败和浪费精力，避免让它发生在你身上吧。举个例子：假设你的团队做出的系统在开发集上表现很好，而在测试集上却表现不佳。如果你的开发集和测试集来自于用一个分布，那么你会知道是过拟合开发集(是否还有可能是训练集过拟合？)了，此时明显的解决办法就是获得更多开发集数据。但是，若开发集和测试集来自不同的分布，那么就不容易找到解决方向了。有几个导致在测试集上表现不佳的原因： 过拟合开发集 测试集比开发集更严格。此时你的算法可能跟预期的一样好了，没有能改进很多的方法了 测试集不一定更严格，只是和开发集不同而已。所以在开发集上表现好的算法不一定在测试集上表现的也好。对于这种情况，你为提高算法在开发集上的表现所做的工作，就没用了。 在“机器学习应用”上工作已经够难了，开发集和测试集不匹配又将会带来不确定性：若算法在开发集上表现提高，是否在测试集上表现也会提高？开发集和测试集的不匹配让我们更难找出什么样的解决方向能奏效，从而使得很难确定工作的优先级。如果你正在处理 3rd party benchmark 问题，这个问题的创建者可能已经指定开发集和测试集来自不同的分布。在这种情况下，运气，而非技术，将会对算法表现有较大影响。如何将“在一个分布上训练的算法能泛化到其他分布上”是一个重要的科研问题。但是，如果你的目标是在一个特定的机器学习应用领域取得进步，而不是使科研问题进步，那么我建议你选择开发集和测试集来自同一分布，这会使你的团队更有效率。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY -- 5.your development and test sets","date":"2017-01-28T02:52:40.000Z","path":"2017/01/28/MLY-5-your-development-and-test-sets/","text":"让我们回想一下猫图片那个例子：你们公司运行一个手机app，用户往app上上传许多不同物体的图片，而你想从这些图片中让机器自动找到猫图片。你的团队从不同的网站上获得了包含猫的图片（正样例）和不包含猫的图片（负样例）。然后将70%的图片用作训练集，30%的图片用作测试集。使用这些图片，你的团队构建了一个猫检测器，这个检测器在训练集和测试集上表现很好。但是，当你将这个分类器发布到移动端app时，其表现却很差。为什么会这样？你发现用户上传的图片与组成训练集的图片有不同的外观：用户上传的图片是用手机拍摄的，因此图片分辨率较低，模糊，并且光线不太理想。因你的训练/测试集都是由网站上的图片组成的，所以由训练集训练除的算法不能很好地泛化到手机图片的分布。在大数据时代之前，在机器学习中，随机70%/30%分割数据集为训练集和测试集是一个常见规则。这种规则可行，但是，在越来越多的“训练时的分布（例如上例网站图片）和最终关心的问题的分布（上例的手机图片）是不同的”应用上， 这种规则却不是一个好主意。我们通常定义： 训练集 – 用来训练你的学习算法 开发集(development set) – 用来调整参数，选择特征，根据学习算法做出其他决定。有时也叫hold-out交叉验证集。 测试集 – 用来评价学习算法的表现，不能将其用于对学习算法和算法参数的选择。 一旦你们定义里开发集和测试集，你的团队就可以尝试不同的ideas（例如，不同的学习算法参数），从而发现哪种ideas表现最好。开发集和测试集能使你的团队快速地看到你们的学习算法表现的怎么样。换句话来说，开发集和测试集能指引你的团队对机器学习系统做出最重要的改变。所以，你应该做到： 选择的开发集和测试集能够反映你期望将来能得到的并且算法在上面表现不错的数据的特点。换句话说，你的测试集不应该只是可利用的数据的30%，特别是将来的数据（移动端app图像）和你的训练数据（网站图像）不同时。如果你还没有发布app，没有用户的你可能不能得到能够精确反映算法将来处理的数据的数据集。但是，你可以尝试得到一些近似数据。例如，拉上你的亲朋好友，让他们用手机拍照片并传给你。一旦你的app发布了，你就能够根据用户的数据更新你的开发/测试数据集啦！假如，你没有任何途径能够得到一些近似数据，你也可以使用网站图像先开始。但你应该意识到，这会导致系统的泛化能力不好。我们需要判断一下，在构建很棒的开发集和测试集上应该投资多少。但是不要妄想训练时的分布和测试的分布相同。尝试挑选能够反映“算法最终能在其上表现好的数据”的样例，而不是你碰巧得到的什么数据（这里的for training应该理解成整个训练过程，包括训练，交叉验证和测试）。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- 4.Scale drivers machine learning progress","date":"2017-01-27T02:39:40.000Z","path":"2017/01/27/MLY-Scale-drivers-machine-learning-progress/","text":"深度学习（神经网络）的很多想法已经存在几十年了，为什么这些想法现在才流行起来？促使机器学习进来的进步的两大因素是： 可得到的数据。现在，人们会花费更多的时间在数字设备上（例如笔记本，手机）。人们的数字行为产生了大量的数据，而这些数据可以用来训练我们的学习算法。 计算的规模。 也就是在近几年，我们才有计算能力训练足够大的神经网络，这种足够大的网络能利用我们拥有的巨大的数据集。 具体来说，即使你积累了更多数据，把这些数据用于训练旧的学习算法（例如逻辑回归），这些算法仍然表现平平。即旧的学习算法的学习曲线是“平坦”的，即使你给它更多数据，它仍然停止增长。这种情况就像旧算法不知道如何处理我们现在拥有的数据似的。对于同样的监督学习任务，如果我们训练一个小的神经网络，将会得到一个稍微好点的结果：这里，小的神经网络是指神经网络的隐藏单元/层/参数较小。如果你训练越来越大的神经网络，你将会得到越来越好的结果[注释1]。因此，当有（i）训练一个非常大的网络（训练过程会遵循上图中的绿色曲线）；(ii)拥有大规模数据 这两个条件满足时，你将得到最好的表现。还有一些其他的细节，例如神经网路的结构也很重要，并且在这个领域中有很多创新工作的提出。但目前来说，提高网络表现性能的相对可靠地方法就是(i)训练更大的网络(ii)收集更多数据。满足条件（i）和（ii）的过程是相当复杂的，本书将详细讨论如何完成这项工作的细节。首先，我们将从既能用于传统学习算法又能用于神经网络的通用策略开始，然后讲述构建深度学习系统的最现代的策略。[注释1]: 下图表示神经网络在小规模数据中表现挺好。但这种好的程度，不如神经网络在大规模数据中的表现好的程度（可以看出随着数据规模增大，Large NN倾斜越大）。在小规模数据中，传统算法表现的好与不好取决于如何手工设计特征。例如，如果你有20个训练样例，选择逻辑回归还是神经网络并不重要，手工设计特征将比选择算法对结果影响更大。但是如果有100万样例，还是选择神经网络把。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- 3.Prerequisites and Notation","date":"2017-01-27T01:47:11.000Z","path":"2017/01/27/MLY-Prerequisites-and-Notation/","text":"如果你学习过机器学习课程，或者有使用监督学习的经验，你将能够更好理解这本书。本书中，我在讲述下面的章节时，会假设你对监督学习很熟悉。监督学习是在有标签的数据(x,y)中学习从x映射到y的一个函数，线性回归、逻辑回归、神经网络都是监督学习算法。目前存在很多机器学习的形式，但机器学习产生的实际价值多数都是监督学习带来的。在这本书中，我将会多次提到神经网络（即深度学习）。为学习这本书，你需要对神经网络（或深度学习）有一个基本的理解。如果你对神经网络（或深度学习）不理解，你可以看Coursera上的机器学习的前三个星期的课程，网址是http://ml-class.org","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- 2.How to use this book to help your team?","date":"2017-01-25T13:47:08.000Z","path":"2017/01/25/Machine-Learning-Yearning-How-to-use-this-book-to-help-your-team/","text":"在读完这本书后，你将会对“如何为机器学习项目设定技术方向”有一个深入的理解。但是你的队友可能不理解你为什么偏向那个技术方向，例如你想让你队友们定义一个单数评价矩阵，但是他们对此不信服。你将会如何说服他们？这就是我将章节设置得如此短的原因：你可以将1-2页的内容打印出来，让你的队友读一读。对事务优先级的改变，将对你的团队的工作效率有很大的影响。多提几个对团队有帮助的改变策略，你就会成为众人眼中的超人啦！","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]},{"title":"MLY翻译 -- 1.Why Machine Learning Strategy?","date":"2017-01-25T13:42:47.000Z","path":"2017/01/25/Machine-Learning-Yearning-Why-Machine-Learning-Strategy/","text":"机器学习是很多应用的基础，如网页搜索，反垃圾邮件，语音识别，推荐系统等。如果你的团队正在做一个基于机器学习的应用，这本书能够帮你的团队取得快速的进步。 一个例子：提供猫图片的创业公司假如你现在正在创建一个为猫奴提供很多猫图片的公司，你用神经网络建立一个从图片中检测猫的计算机视觉系统。但悲剧的是，你发现你的学习算法精度不够，面临提高猫检测器精度的巨大压力，你要怎么做？你的小团队很给力，他们提出以下解决问题的方向： 收集更多数据：收集更多猫的图片 收集多样化训练数据：例如，收集具有不同姿势的猫图片；收集具有不同毛色的猫图片；收集拍摄时相机参数设置不同的图片 训练时间长一点儿：将梯度下降法的循环次数设置多一点儿 尝试更大的神经网络，所谓更大是指层数更多或者说隐藏单元更多或者说权重参数更多。 尝试更小的神经网络 尝试添加正则化项（例如L2正则） 改变神经网络的结构（例如改变激活函数，隐藏单元等） …… 在以上所述的方向中，如果能你能选择一个正确的方向，你将会建立一个领先于他人的猫图片平台，你的创业公司也会成功。若你选择一个不好的方向，你可能会浪费几个月。你将如何处理？这本书将会告诉你如何处理。对于多数机器学习问题，我们都能找到一些线索，这些线索能够告诉我们什么是有效的尝试，什么是无效的尝试。学会解读这些线索，将可能节约你数月甚至数年的开发时间。","tags":[{"name":"翻译","slug":"翻译","permalink":"https://mengmengmiao.github.io/tags/翻译/"}]}]